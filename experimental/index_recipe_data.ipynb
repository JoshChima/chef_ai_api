{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Indexing Recipes Using LazyGraphRag\n",
    "This notebook demonstrates how to index using the LazyGraphRag library.\n",
    "\n",
    "Learn more about LazyGraphRag here: [GraphRAG](https://datastax.github.io/graph-rag/examples/lazy-graph-rag/?h=lazy)\n",
    "\n",
    "## Datasets\n",
    "The datasets used in this notebook are:\n",
    "- **CookingRecipes Dataset**:\n",
    "    source: https://huggingface.co/datasets/CodeKapital/CookingRecipes\n",
    "    description: A dataset of cooking recipes with ingredients, directions, and other relevant information.\n",
    "- **Q&A For Recipes Dataset**:\n",
    "    source: https://huggingface.co/datasets/Hieu-Pham/cooking_squad\n",
    "    description: A dataset of cooking-related questions and answers to help users troubleshoot issues with recipe directions. The context of the questions are recipes from the CookingRecipes dataset.\n",
    "- **General preference Q&A Dataset**:\n",
    "    source: https://huggingface.co/datasets/andrewsiah/se_cooking_preference_sft\n",
    "    description: A dataset of questions and answers to help better inform users about cooking techniques and ingredients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(dotenv_path='../.env')\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --extra-index-url=https://pypi.nvidia.com cudf-cu12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### This is a speed test for reading a parquet file with Pandas and cuDF\n",
    "# import cudf\n",
    "# import pandas as pd\n",
    "# import time\n",
    "# # Pandas\n",
    "# pd_start = time.time()\n",
    "# pf_df = pd.read_parquet(\"./data/cooking_recipes.parquet\")\n",
    "# # pf_df[\"tip_percentage\"] = pf_df[\"tip\"] / pf_df[\"total_bill\"] * 100\n",
    "# print(pf_df.shape)\n",
    "# pd_end = time.time()\n",
    "# print(f\"Pandas read_csv took {pd_end - pd_start} seconds\")\n",
    "\n",
    "# # cuDF\n",
    "# cudf_start = time.time()\n",
    "# cudf_df = cudf.read_parquet(\"./data/cooking_recipes.parquet\")\n",
    "# # cudf_df[\"tip_percentage\"] = cudf_df[\"tip\"] / cudf_df[\"total_bill\"] * 100\n",
    "# print(cudf_df.shape)\n",
    "# cudf_end = time.time()\n",
    "# print(f\"cuDF read_csv took {cudf_end - cudf_start} seconds\")\n",
    "\n",
    "############################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Recipe Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext cudf.pandas\n",
    "# %reload_ext cudf.pandas\n",
    "import requests \n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "\n",
    "n=250000\n",
    "# check if the data is saved\n",
    "if os.path.exists('./data/cooking_recipes.parquet.gzip'):\n",
    "    recipes_df = pd.read_parquet('./data/cooking_recipes.parquet.gzip').head(n)\n",
    "else:\n",
    "    r = requests.get(\"https://huggingface.co/api/datasets/CodeKapital/CookingRecipes/parquet/default/train\")\n",
    "    urls = [{'url': url, 'file_name': url.split('/')[-1]} for url in r.json()]\n",
    "\n",
    "    for url in urls:\n",
    "        print(url['url'])\n",
    "        url['df'] = pl.read_parquet(url['url']).to_pandas()\n",
    "\n",
    "    recipes_df = pd.concat([url['df'] for url in urls]).reindex()\n",
    "\n",
    "    # only keep top 1000 rows\n",
    "    recipes_df = recipes_df.head(n)\n",
    "    # save the data to parquet in /data\n",
    "    recipes_df.to_parquet('./data/cooking_recipes.parquet.gzip', compression='gzip')\n",
    "\n",
    "# rename the columns 'Unnamed: 0' to 'i64'\n",
    "recipes_df.rename(columns={'Unnamed: 0':'i64'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recipes_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "regex = re.compile('[^a-zA-Z]')\n",
    "\n",
    "# recipes_df = recipes_data.copy()\n",
    "\n",
    "# filter out rows with empty 'title' column\n",
    "recipes_df = recipes_df[recipes_df['title'].notnull()]\n",
    "\n",
    "# drop duplcates on the 'titlee' column\n",
    "recipes_df.drop_duplicates(subset='title', inplace=True)\n",
    "# rename the column \"NER\" to \"ner\"\n",
    "recipes_df.rename(columns={'NER':'ner'}, inplace=True)\n",
    "\n",
    "# create an id column that is a combination of the title and i64. title should be lowercased and spaces replaced with underscores. # Remove any non-alphanumeric characters from the id column\n",
    "recipes_df['source_id'] = recipes_df.apply(lambda x: f\"{x['i64']}_{regex.sub('', x['title']).lower().replace(' ', '_')}\", axis=1)\n",
    "# apply json.load to directions, ingredients and NER columns\n",
    "recipes_df['directions'] = recipes_df['directions'].apply(json.loads)\n",
    "recipes_df['ingredients'] = recipes_df['ingredients'].apply(json.loads)\n",
    "recipes_df['ner'] = recipes_df['ner'].apply(json.loads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: The AI will recieve recipe context formatted as Markdown.\n",
    "\n",
    "# Create a function that takes a row fro the recipe dataset and returns a string in markdown format\n",
    "recipe_format_md = lambda r: \"\"\"# {title}\n",
    "\n",
    "## Ingredients\n",
    "- {ingredients}\n",
    "\n",
    "## Directions\n",
    "- {directions}\n",
    "\"\"\".format(title=r['title'], ingredients='\\n- '.join(r['ingredients']), directions='\\n- '.join(r['directions']))\n",
    "\n",
    "recipes_df['md'] = recipes_df.apply(recipe_format_md, axis=1)\n",
    "\n",
    "\n",
    "#########\n",
    "# uncomment the following line to test the function on a single row\n",
    "# print(recipes_df['md'][0])\n",
    "#########\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Named Entities in Recipe Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_ner_data(recipes_df):\n",
    "    # Explode the 'ner' column into individual rows\n",
    "    recipes_df = recipes_df[['source_id', 'ner']].explode('ner')\n",
    "    # Aggregate source_id in an array by ner\n",
    "    recipes_df = recipes_df.groupby('ner')['source_id'].apply(list).reset_index()\n",
    "    # Filter out rows in 'ner' with single alphanumeric character\n",
    "    recipes_df = recipes_df[recipes_df['ner'].str.len() > 1]\n",
    "    # Trim leading and trailing whitespaces from 'ner' strings\n",
    "    recipes_df['ner'] = recipes_df['ner'].str.strip()\n",
    "    # # Remove leading \"'s\" or \",\" from ner strings\n",
    "    # recipes_df['ner'] = recipes_df['ner'].str.replace(r\"^['s,]+\", '', regex=True)\n",
    "    # Filter 'A.' from the ner column\n",
    "    recipes_df = recipes_df[recipes_df['ner'] != 'A.']\n",
    "    # Add column with length of 'ner' string\n",
    "    recipes_df['ner_str_len'] = recipes_df['ner'].str.len()\n",
    "    # Sort by column: 'ner_str_len' (ascending)\n",
    "    recipes_df = recipes_df.sort_values(['ner_str_len'])\n",
    "    # Filter rows where the ner_str_len < 3\n",
    "    recipes_df = recipes_df[recipes_df['ner_str_len'] > 2]\n",
    "    # Capitalize first characters of 'ner' strings\n",
    "    recipes_df['ner'] = recipes_df['ner'].str.capitalize()\n",
    "    # Explode source_id column into individual rows\n",
    "    recipes_df = recipes_df.explode('source_id')[['source_id', 'ner']]\n",
    "    # Aggregate source_id in an array by ner again\n",
    "    recipes_df = recipes_df.groupby('ner')['source_id'].apply(list).reset_index()\n",
    "    # Count items in source_id list\n",
    "    recipes_df['item_count'] = recipes_df['source_id'].apply(lambda x: len(x))\n",
    "    # Sort by column: 'item_count' (descending)\n",
    "    recipes_df = recipes_df.sort_values(['item_count'], ascending=[False])\n",
    "    # Filter out rows where ner == 'Alt'\n",
    "    # recipes_df = recipes_df[recipes_df['ner'] != 'Alt']\n",
    "    return recipes_df\n",
    "\n",
    "ner_df = clean_ner_data(recipes_df.copy())\n",
    "# ner_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use SpaCy to Consolodate Named Entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy \n",
    "\n",
    "spacy.prefer_gpu()\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "# ner_df['ner_token'] = ner_df['ner'].apply(lambda x: nlp(x))\n",
    "ner_df['ner_token'] = list(nlp.pipe(ner_df['ner'], disable=[\"tagger\", \"parser\", \"ner\", \"attribute_ruler\", \"lemmatizer\"]))\n",
    "\n",
    "\n",
    "# # has_vector attribute\n",
    "ner_df['ner_token_has_vector'] = ner_df['ner_token'].apply(lambda x: x.has_vector)\n",
    "# Filter out rows where ner_token_has_vector == False\n",
    "ner_df = ner_df[ner_df['ner_token_has_vector'] == True]\n",
    "# # vector_norm attribute\n",
    "# ner_df['ner_token_vector_norm'] = ner_df['ner_token'].apply(lambda x: x.vector_norm)\n",
    "# # is_oov attribute\n",
    "ner_df['ner_token_is_oov'] = ner_df['ner_token'].apply(lambda x: x.is_oov if hasattr(x, 'is_oov') else None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "# TODO: Partition the data into chunks of 1000 rows each and save each chunk to a separate parquet file in /data. \n",
    "# cross join each partition with every other partition at least once and calculate the similarity between the NER tokens in each row. \n",
    "# After each cross join, save the resulting data to a parquet file in /data.\n",
    "\n",
    "# # Partition the data into chunks of 3000 rows each\n",
    "ner_df_partitions = [ner_df.iloc[i:i+3000] for i in range(0, len(ner_df), 3000)]\n",
    "ner_df_len = len(ner_df_partitions)\n",
    "print(ner_df_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Warning: This cell will take a long time to run\n",
    "\n",
    "from tqdm.notebook import trange, tqdm\n",
    "ner_similarity_partitions = []\n",
    "for i in trange(ner_df_len):\n",
    "    part_ner_df_1 = ner_df_partitions[i]\n",
    "    for j in trange(ner_df_len):\n",
    "        save_name = f'./data/similarity_scores/ner_similarity_{i}_{j}.parquet.gzip'\n",
    "        if os.path.exists(save_name):\n",
    "            cross_join_df = pd.read_parquet(save_name)\n",
    "        else:\n",
    "            part_ner_df_2 = ner_df_partitions[j]\n",
    "            # # Cross join each partition with every other partition at least once\n",
    "            cross_join_df = pd.merge(part_ner_df_1[['ner', 'ner_token']], part_ner_df_2[['ner', 'ner_token']], how='cross', suffixes=('_1', '_2'))\n",
    "            # Calculate the similarity between the NER tokens in each row\n",
    "            cross_join_df['similarity'] = cross_join_df.apply(lambda x: x['ner_token_1'].similarity(x['ner_token_2']), axis=1)\n",
    "            # filter out rows where similarity < 0.99\n",
    "            cross_join_df = cross_join_df[cross_join_df['similarity'] > 0.99]\n",
    "            # Save the resulting data to a parquet file in /data. Only all columns except ner_token_1 and ner_token_2\n",
    "            saved_columns = [col for col in cross_join_df.columns if col.startswith('ner_token') == False]\n",
    "            cross_join_df[saved_columns].to_parquet(save_name, compression='gzip')\n",
    "        ner_similarity_partitions.append(cross_join_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_grouped_similarities = pd.concat(ner_similarity_partitions)\n",
    "\n",
    "# Aggregate ner_2 in an array by ner_1\n",
    "ner_grouped_similarities = ner_grouped_similarities.groupby('ner_1')['ner_2'].apply(list).reset_index()\n",
    "\n",
    "# join ner_grouped_similarities with ner_df on ner column\n",
    "ner_grouped_similarities = pd.merge(ner_grouped_similarities, ner_df[['ner', 'source_id']], left_on='ner_1', right_on='ner')\n",
    "# Explode ner_2 column into individual rows\n",
    "ner_grouped_similarities = ner_grouped_similarities.explode('ner_2')[['source_id', 'ner_1', 'ner_2']]\n",
    "# rename source_id to ner_source_id\n",
    "ner_grouped_similarities.rename(columns={'source_id':'ner_1_source_id'}, inplace=True)\n",
    "# join ner_grouped_similarities with ner_df on ner_2 column\n",
    "ner_grouped_similarities = pd.merge(ner_grouped_similarities, ner_df[['ner', 'source_id']], left_on='ner_2', right_on='ner')\n",
    "# rename source_id to ner_2_source_id\n",
    "ner_grouped_similarities.rename(columns={'source_id':'ner_2_source_id'}, inplace=True)\n",
    "# Drop ner column\n",
    "ner_grouped_similarities.drop(columns=['ner'], inplace=True)\n",
    "# rename ner_1 to ner\n",
    "ner_grouped_similarities.rename(columns={'ner_1':'ner'}, inplace=True)\n",
    "# combine ner_1_source_id and ner_2_source_id into a single column\n",
    "ner_grouped_similarities['source_id'] = ner_grouped_similarities.apply(lambda x: x['ner_1_source_id'] + x['ner_2_source_id'], axis=1)\n",
    "# drop ner_2, ner_1_source_id and ner_2_source_id columns\n",
    "ner_grouped_similarities.drop(columns=['ner_1_source_id', 'ner_2', 'ner_2_source_id'], inplace=True)\n",
    "# concatenate ner_df and ner_grouped_similarities\n",
    "cleaned_ner_df = pd.concat([ner_df[['ner', 'source_id']], ner_grouped_similarities])\n",
    "\n",
    "\n",
    "# Remove \"'s\" from the beginning of ner strings\n",
    "cleaned_ner_df['ner'] = cleaned_ner_df['ner'].str.replace(r\"^'s\", '', regex=True)\n",
    "# Remove quotation marks from the 'ner' column\n",
    "cleaned_ner_df['ner'] = cleaned_ner_df['ner'].str.replace('\"', '', regex=False)\n",
    "# Remove specific characters at the start of 'ner'\n",
    "cleaned_ner_df['ner'] = cleaned_ner_df['ner'].str.lstrip('()+/:,.')\n",
    "# Trim white spaces in the 'ner' column\n",
    "cleaned_ner_df['ner'] = cleaned_ner_df['ner'].str.strip()\n",
    "# Capitalize the first letter of 'ner' strings\n",
    "cleaned_ner_df['ner'] = cleaned_ner_df['ner'].str.capitalize()\n",
    "# explode source_id column into individual rows\n",
    "cleaned_ner_df = cleaned_ner_df.explode('source_id')\n",
    "# Aggregate ner in an array by source_id\n",
    "cleaned_ner_df = cleaned_ner_df.groupby('source_id')['ner'].apply(list).reset_index()\n",
    "# Rename ner to cleaned_ners\n",
    "cleaned_ner_df.rename(columns={'ner':'cleaned_ner'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join recipes_df with ner_grouped_similarities on source_id\n",
    "recipes_df = pd.merge(recipes_df, cleaned_ner_df, on='source_id', how='left')\n",
    "# fill NaN values in cleaned_ners column with empty list\n",
    "recipes_df['cleaned_ner'] = recipes_df['cleaned_ner'].apply(lambda x: x if isinstance(x, list) else [])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recipe Documents\n",
    "Before loading the recipe data, we need to prepare the recipe documents.\n",
    "The `page_content` will be the Markdown representation of the recipe.\n",
    "LazyGraphRag will generate the graph edges using metadata from the recipe documents:\n",
    "- `keywords`: The CookingRecipes dataset came with a `ner` field that contains entities extracted from the recipe. These entities would be ingredients found in the recipe.\n",
    "- `source_id`: The unique identifier for the recipe.\n",
    "- `type`: All recipes will have the type `recipe`. This will help distinguish the recipe nodes from other nodes in the graph, such as the question-answer nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "# convert the recipes to langchain documents\n",
    "recipe_docs = [Document(page_content=r['md'], id=r['source_id'], metadata={'keywords':r['cleaned_ner'], 'source_id': r['source_id'], 'type':'recipe'}) for r in recipes_df.to_dict(orient='records')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Cooking Q&A W/ Recipe Context\n",
    "As discussed earlier, [Hieu-Pham's dataset](https://huggingface.co/datasets/Hieu-Pham/cooking_squad) contains questions and answers related to the CookingRecipes dataset. We will use this dataset to generate the question-answer nodes in the graph. We will leverage the connection between the recipe and the question-answer nodes to generate the graph edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipe_qa_df = pd.read_json(\"hf://datasets/Hieu-Pham/cooking_squad/squad_cooking_transformed.json\")\n",
    "\n",
    "# explode the 'answers' column\n",
    "recipe_qa_df['answer_start'] = recipe_qa_df['answers'].apply(lambda x: x['answer_start'])\n",
    "recipe_qa_df['answer'] = recipe_qa_df['answers'].apply(lambda x: x['text'])\n",
    "\n",
    "# drop the initial 'answers' column\n",
    "recipe_qa_df.drop(columns=['answers'], inplace=True)\n",
    "\n",
    "# Grab the title of the column from splitting the 'context' column on the first '\\n'\n",
    "recipe_qa_df['title'] = recipe_qa_df['context'].apply(lambda x: x.split('\\n')[0])\n",
    "# drop the original 'context' column\n",
    "recipe_qa_df.drop(columns=['context'], inplace=True)\n",
    "\n",
    "# join recipe_qa_df with recipes_df on the 'title' column. keep all rows in recipe_qa_df\n",
    "recipe_qa_df = recipe_qa_df.merge(recipes_df[['title', 'source_id', 'md']], on='title', how='left')\n",
    "\n",
    "# rename the 'md' column to 'context'\n",
    "recipe_qa_df.rename(columns={'md':'context'}, inplace=True)\n",
    "\n",
    "# format the qa pairs in markdown for the AI\n",
    "qa_format_md = lambda qa: \"\"\"\n",
    "<question>\n",
    "{question} \n",
    "</question>\n",
    "\n",
    "<answer>\n",
    "{answer} \n",
    "</answer>\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\"\"\".format(question=qa['question'], answer=qa['answer'], context=qa['context'])\n",
    "\n",
    "recipe_qa_df['md'] = recipe_qa_df.apply(qa_format_md, axis=1)\n",
    "\n",
    "# rename the 'id' column to 'qa_id'\n",
    "recipe_qa_df.rename(columns={'id':'qa_id'}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipe_qa_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question & Answer Documents\n",
    "Before loading the recipe data, we need to prepare the documents again.\n",
    "The `page_content` will be the Markdown representation of the Q&A.\n",
    "LazyGraphRag will generate the graph edges using metadata from the documents:\n",
    "- `source_id`: The unique identifier for the recipe context linked to the document.\n",
    "- `type`: All Q&A documents will have the type `question-answer`. This will help distinguish the nodes from other nodes in the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Prepare Question-Answer Document\n",
    "recipe_qa_docs = [Document(page_content=qa['md'], id=qa['qa_id'], metadata={'source_id': qa['source_id'], 'type':'question-answer'}) for qa in recipe_qa_df.to_dict(orient='records')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Populating the Vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma.vectorstores import Chroma\n",
    "from langchain_graph_retriever.transformers import ShreddingTransformer\n",
    "\n",
    "#########\n",
    "# If you want to only store the recipe documents, uncomment the following variable assignment and comment the one below it\n",
    "# vector_store = Chroma.from_documents(\n",
    "#     documents=list(ShreddingTransformer().transform_documents(recipe_docs)),\n",
    "#     embedding=embeddings,\n",
    "#     collection_name=\"recipes\",\n",
    "#     persist_directory=\"./data/recipes_chroma_db\"\n",
    "# )\n",
    "#########\n",
    "shredder = ShreddingTransformer() \n",
    "vector_store = Chroma.from_documents(\n",
    "    documents=list(shredder.transform_documents(recipe_docs + recipe_qa_docs)),\n",
    "    embedding=embeddings,\n",
    "    collection_name=\"recipe_qa_combined\",\n",
    "    persist_directory=\"./data/recipe_qa_combined_chroma_db\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Traversal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graph_retriever.strategies import Eager\n",
    "from langchain_graph_retriever import GraphRetriever\n",
    "from langchain_graph_retriever.adapters.chroma import ChromaAdapter\n",
    "\n",
    "traversal_retriever = GraphRetriever(\n",
    "    store = ChromaAdapter(vector_store, shredder, {\"keywords\"}),\n",
    "    edges = [(\"keywords\", \"keywords\"), (\"source_id\", \"source_id\")],\n",
    "    strategy = Eager(k=5, start_k=2, max_depth=3),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the retrieval on a single question. This should return relevant recipes and their context\n",
    "results = traversal_retriever.invoke(\"I'm in Ohio and I just had a small round chocolate that had peanut butter. I can't remeber the name of it. All I remember is that it had an 'eye' in the name. If you find it, get me the recipe\")\n",
    "#########\n",
    "# If you want to test the retrieval on a single question that test the retrieval of a Q&A on a specific recipe, uncomment the following line\n",
    "# results = traversal_retriever.invoke(\"No Bake Cookies: How long should the clusters stand until the firm up?\")\n",
    "#########\n",
    "for doc in results:\n",
    "    print(f\"{doc.id}:\\n{doc.page_content}\")\n",
    "    print(doc.metadata.get('keywords', []))\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use within a chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "llm = init_chat_model(\"gpt-4o-mini\", model_provider=\"openai\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"Answer the question based only on the context provided.\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(\n",
    "        f\"text: {doc.page_content} metadata: {doc.metadata}\" for doc in docs\n",
    "    )\n",
    "\n",
    "\n",
    "# chain = (\n",
    "#     {\"sources\": traversal_retriever}\n",
    "#     | {\"context\": RunnableLambda(lambda x: format_docs(x['sources'])), \"question\": RunnablePassthrough()}\n",
    "#     | prompt\n",
    "#     | llm\n",
    "#     | StrOutputParser()\n",
    "# )\n",
    "\n",
    "chain = (\n",
    "    {\"context\": traversal_retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    # | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# response = chain.invoke(\"I'm in Ohio and I just had a small round chocolate that had peanut butter. I can't remeber the name of it. All I remember is that it had an 'eye' in the name. If you find it, get me the recipe\")\n",
    "# response = chain.invoke(\"What are some recipe that use chocolate and creamcheese? Give me the recipes\")\n",
    "response = chain.invoke(\"I'm looking for some seafood recipes. Can you help me?\")\n",
    "# response = chain.invoke(\"I'm looking for some chili recipes that use pork tenderloin?\")\n",
    "# response = chain.invoke(\"What is the id for the recipe Fruit Medley?\")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response.model_dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chain.invoke(\"Get me the recipe for Seafood And Pasta Salad\")\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
